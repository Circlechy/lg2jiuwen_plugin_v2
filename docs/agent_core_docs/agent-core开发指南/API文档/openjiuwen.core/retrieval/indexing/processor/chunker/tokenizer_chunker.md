# openjiuwen.core.retrieval.indexing.processor.chunker.tokenizer_chunker

## class openjiuwen.core.retrieval.indexing.processor.chunker.tokenizer_chunker.TokenizerChunker

基于tokenizer的固定大小分块器。

### __init__

```python
__init__(chunk_size: int, chunk_overlap: int, tokenizer: Any, **kwargs: Any)
```

初始化基于tokenizer的分块器。

**参数**：

* **chunk_size**(int)：分块大小（token数）。默认值：无。
* **chunk_overlap**(int)：分块重叠大小（token数）。默认值：无。
* **tokenizer**(Any)：分词器，必须具有encode和decode方法。默认值：无。
* **kwargs**(Any)：可变参数，用于传递其他额外的配置参数。

### chunk_text

```python
chunk_text(text: str) -> List[str]
```

分块文本。

**参数**：

* **text**(str)：待分块的文本。默认值：无。

**返回**：

**List[str]**，返回分块后的文本列表。

