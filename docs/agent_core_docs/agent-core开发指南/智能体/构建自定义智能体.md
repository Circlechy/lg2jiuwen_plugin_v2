本章节演示了如何基于openJiuwen快速开发自定义智能体。该示例构建的智能体能够调用大模型服务，并且识别大模型输出结果中可能包含的敏感词。通过示例，你将会了解到如下信息：

- 如何继承`AgentConfig`基类，实现自定义智能体的配置类。
- 如何重写`BaseAgent`基类的初始化方法`__init__`，实现自定义智能体的初始化方法。
- 如何实现`BaseAgent`基类定义的批执行抽象接口`invoke`，异步批执行自定义智能体的具体业务逻辑。

# 应用设计流程

本章节设计了一个自定义智能体，实现了一个更安全的智能体：能够基于预置敏感词校验输出答案。为了实现该智能体，开发者需要：

- 设计自定义智能体的配置类。
- 在智能体的初始化方法`__init__`，向智能体配置预设的敏感词列表。
- 在自定义智能体的批执行抽象接口`invoke`中，实现智能体的业务逻辑：结合智能体的默认系统提示词和用户请求，构造调用大模型输入；进而调用大模型服务，并等待获取大模型的返回；最终基于智能体配置的敏感词列表，校验大模型输出，保障智能体不输出恶意信息。

  <div align="center">
    <img src="../images/MyAgent.png" alt="MyAgent" width="70%">
  </div>

# 前提条件

Python的版本应高于或者等于Python 3.11版本，建议使用3.11.4版本，使用前请检查Python版本信息。

# 安装openJiuwen

用户可选择创建虚拟环境进行openJiuwen的安装，安装命令如下：

```bash
pip install -U openjiuwen
```

# 实现自定义智能体的配置类

为了实现一个智能体能够调用大模型服务，并且识别大模型输出结果中可能包含的敏感词，除了`AgentConfig`中定义的基础配置项（如智能体的`id`、`version`、`description`等），还需要添加自定义配置（具体包括自定义调用大模型的配置信息`model`和自定义的敏感词列表`sensitive_words`）。示例代码如下：

```python
import os
from typing import Optional, List
from pydantic import Field
from openjiuwen.agent.config.base import AgentConfig
from openjiuwen.core.utils.llm.base import BaseModelInfo
from openjiuwen.core.component.common.configs.model_config import ModelConfig


class MyAgentConfig(AgentConfig):
    model: Optional[ModelConfig] = Field(default=None)
    sensitive_words: List[str] = Field(default_factory=list)


def create_model_config() -> ModelConfig:
    """通过环境变量获取大模型相关的配置信息"""
    return ModelConfig(
        model_provider=os.getenv("MODEL_PROVIDER", ""),
        model_info=BaseModelInfo(
            model=os.getenv("MODEL_NAME", ""),
            api_base=os.getenv("API_BASE", ""),
            api_key=os.getenv("API_KEY", ""),
            temperature=0.7,
            top_p=0.9,
            timeout=30,
        ),
    )

# 创建自定义智能体配置信息的对象
my_agent_config = MyAgentConfig(
    id="my_agent_id",
    version="0.0.1",
    description="我的自定义智能体",
    model=create_model_config(),  # 配置大模型相关参数信息
    sensitive_words=["自定义敏感词列表"]
)
```

其中，大模型服务的配置信息通过`create_model_config`方法读取环境变量，包括模型提供商（`MODEL_PROVIDER`）、模型名称（`MODEL_NAME`）、大模型服务调用路径（`API_BASE`）和大模型服务认证鉴权信息（`API_KEY`），完成对大模型服务调用对象的配置。

# 实现初始化方法

接下来，开发者需要实现自定义智能体的初始化方法，支持创建自定义智能体的对象。`BaseAgent`基类的初始化方法为智能体提供了初始化运行时`Runtime`、配置管理`Config`的能力。除此之外，自定义智能体除了调用`BaseAgent`基类的初始化方法，还需要根据大模型的配置信息、创建大模型调用对象，初始化配置的敏感词列表。具体示例如下：

```python
from openjiuwen.agent.config.base import AgentConfig
from openjiuwen.core.agent.agent import BaseAgent
from openjiuwen.core.utils.llm.model_utils.model_factory import ModelFactory

class MyAgent(BaseAgent):
    def __init__(self, agent_config : AgentConfig):
        # 初始化配置信息
        super().__init__(agent_config)
        # 获取大模型配置
        self._model_config = agent_config.model
        # 使用ModelFactory 创建大模型调用对象
        self._llm = ModelFactory().get_model(
            model_provider=self._model_config.model_provider,
            api_key=self._model_config.model_info.api_key,
            api_base=self._model_config.model_info.api_base
        )
        # 初始化配置自定义敏感词列表
        self._sensitive_words = agent_config.sensitive_words
```

其中，通过`ModelFactory`的`get_model`方法，基于`ModelConfig`中的配置信息，创建大模型实例。

# 实现invoke方法

本示例中的智能体具备以下功能：能够调用大模型服务，并且识别大模型输出结果中可能包含的敏感词。

`invoke`方法用于调用大模型。调用前先将用户输入封装为`HumanMessage`，结合自定义智能体自带的系统提示词`SystemMessage`，作为大模型的输入，得到大模型的返回结果。最后校验大模型的输入中是否包含了自定义的敏感词，一旦大模型输出中含有自定义的敏感词，则输出一段固定话术：对不起，无法回答您的问题。示例代码如下：

```python
from typing import Dict
from openjiuwen.core.agent.agent import BaseAgent
from openjiuwen.core.runtime.runtime import Runtime
from openjiuwen.core.utils.llm.messages import HumanMessage, SystemMessage

class MyAgent(BaseAgent):
    async def invoke(self, inputs: Dict, runtime: Runtime = None):
        # inputs是自定义智能体的输入，以llm_inputs为键，用户输入信息作为用户提示词
        user_message = HumanMessage(content=inputs.get("llm_inputs", "")).model_dump(exclude_none=True)
        # 自定义智能体的默认系统提示词
        system_message = SystemMessage(content="你是一个AI助手").model_dump(exclude_none=True)
        # 大模型输入包括：系统提示词和用户提示词
        llm_input_messages = [system_message, user_message]
        # 调用模型的异步执行方法方法得到模型的输出
        res = await self._llm.ainvoke(model_name=self._model_config.model_info.model_name, messages=llm_input_messages)
        llm_output = res.content
        for word in self._sensitive_words:
            if word in llm_output:
                return "对不起，无法回答您的问题。"
        # 如果大模型输出不包含敏感词，则直接返回大模型输出内容
        return llm_output
```
# 实现stream方法

`stream`方法用于实现智能体的流式调用能力，返回异步迭代器以支持实时响应。在本示例中，为简化实现，该方法直接复用`invoke`的完整处理逻辑，将最终结果以单次流式块形式返回。示例代码如下：

```python
from typing import AsyncIterator, Any

class MyAgent(BaseAgent):
    async def stream(self, inputs: Dict, runtime: Runtime = None) -> AsyncIterator[Any]:
        content = await self.invoke(inputs)
        yield {"type": "answer", "content": content}
```

# 运行自定义智能体

开发者完成自定义智能体的相关实现后，可以调用`invoke`方法、实现异步非流式地运行自定义智能体，示例代码如下：

```python
import asyncio

# 创建自定义智能体的对象
my_agent = MyAgent(my_agent_config)

inputs = {"llm_inputs": "写一个笑话"}
res = asyncio.run(my_agent.invoke(inputs))
print(res)
```

# 完整示例代码

完整的示例参考如下所示：

```python
import asyncio
import os
from typing import Optional, List, Dict, AsyncIterator, Any
from pydantic import Field

from openjiuwen.agent.config.base import AgentConfig
from openjiuwen.core.agent.agent import BaseAgent
from openjiuwen.core.runtime.runtime import Runtime
from openjiuwen.core.utils.llm.base import BaseModelInfo
from openjiuwen.core.component.common.configs.model_config import ModelConfig
from openjiuwen.core.utils.llm.messages import HumanMessage, SystemMessage
from openjiuwen.core.utils.llm.model_utils.model_factory import ModelFactory


def create_model_config() -> ModelConfig:
    """通过环境变量获取大模型相关的配置信息"""
    return ModelConfig(
        model_provider=os.getenv("MODEL_PROVIDER", ""),
        model_info=BaseModelInfo(
            model=os.getenv("MODEL_NAME", ""),
            api_base=os.getenv("API_BASE", ""),
            api_key=os.getenv("API_KEY", ""),
            temperature=0.7,
            top_p=0.9,
            timeout=30,
        ),
    )


class MyAgentConfig(AgentConfig):
    model: Optional[ModelConfig] = Field(default=None)
    sensitive_words: List[str] = Field(default_factory=list)


class MyAgent(BaseAgent):
    def __init__(self, agent_config : AgentConfig):
        # 初始化配置信息
        super().__init__(agent_config)
        # 获取大模型配置
        self._model_config = agent_config.model
        # 使用ModelFactory 创建大模型调用对象
        self._llm = ModelFactory().get_model(
            model_provider=self._model_config.model_provider,
            api_key=self._model_config.model_info.api_key,
            api_base=self._model_config.model_info.api_base
        )
        # 初始化配置自定义敏感词列表
        self._sensitive_words = agent_config.sensitive_words

    async def invoke(self, inputs: Dict, runtime: Runtime = None):
        # inputs是自定义智能体的输入，以llm_inputs为键，用户输入信息作为用户提示词
        user_message = HumanMessage(content=inputs.get("llm_inputs", "")).model_dump(exclude_none=True)
        # 自定义智能体的默认系统提示词
        system_message = SystemMessage(content="你是一个AI助手").model_dump(exclude_none=True)
        # 大模型输入包括：系统提示词和用户提示词
        llm_input_messages = [system_message, user_message]
        # 调用模型的异步执行方法方法得到模型的输出
        res = await self._llm.ainvoke(model_name=self._model_config.model_info.model_name, messages=llm_input_messages)
        llm_output = res.content
        for word in self._sensitive_words:
            if word in llm_output:
                return "对不起，无法回答您的问题。"
        # 如果大模型输出不包含敏感词，则直接返回大模型输出内容
        return llm_output

    async def stream(self, inputs: Dict, runtime: Runtime = None) -> AsyncIterator[Any]:
        content = await self.invoke(inputs)
        yield {"type": "answer", "content": content}


async def main():
    # 创建自定义智能体配置信息的对象
    my_agent_config = MyAgentConfig(
        id="my_agent_id",
        version="0.0.1",
        description="我的自定义智能体",
        model=create_model_config(),  # 配置大模型相关参数信息
        sensitive_words=["自定义敏感词列表"]
    )

    # 创建自定义智能体的对象
    my_agent = MyAgent(my_agent_config)

    inputs = {"llm_inputs": "写一个笑话"}
    res = await my_agent.invoke(inputs)
    print(res)


if __name__ == "__main__":
    asyncio.run(main())
```

如果笑话中包含了自定义的敏感词，则最终输出结果为：

```python
对不起，无法回答您的问题。
```

反之则输出大模型生成的笑话一则。

```python
问：小熊为什么不上班？答：因为它熊啊。
```
