用户可自定义接入模型，需先获取模型请求的URL地址（api_base）和鉴权Token（api_key）。模型接入操作通常需要完成以下三个主要流程：

1. 创建模型文件类：在`openjiuwen/core/utils/llm/model_library`目录下创建自定义模型文件。
2. 实现自定义模型类：其流程一般包括处理模型请求参数、发送模型请求、处理模型返回的结果数据。
3. 使用自定义模型类：创建模型实例，调用模型提供方法。

下面以已经支持的硅基流动厂商siliconflow为例，介绍如何实现自定义模型的接入。

## 创建文件并配置环境变量

在`openjiuwen/core/utils/llm/model_library`目录下创建`siliconflow.py`文件。

## 实现自定义模型类

实现自定义模型子类`Siliconflow`，继承自父类`BaseModelClient`。初始化配置大模型服务鉴权Token`api_key`、大模型服务URL地址`api_base`等参数。并实现父类的抽象方法_invoke、_ainvoke、_stream、_astream（使用RequestChatModel的实现），用于实际调用模型接口获取响应结果，并将结果封装为不同形式的数据类型（如同步调用、异步调用、流式调用等），以满足不同场景下的使用需求。

```python
from typing import List, Dict, Any, Iterator, AsyncIterator
from pydantic import BaseModel
from openjiuwen.core.utils.llm.base import BaseModelClient
from openjiuwen.core.utils.llm.messages import AIMessage
from openjiuwen.core.utils.llm.messages_chunk import AIMessageChunk
from openjiuwen.core.utils.llm.model_utils.default_model import RequestChatModel

class Siliconflow(BaseModel, BaseModelClient):
    _request_model: RequestChatModel = None

    def __init__(self,
                 api_key: str, api_base: str, max_retrie: int = 3, timeout: int = 60, **kwargs):
        super().__init__(api_key=api_key, api_base=api_base, max_retrie=max_retrie, timeout=timeout)
        self._request_model = RequestChatModel(api_key=api_key, api_base=api_base,
                                               max_retrie=max_retrie, timeout=timeout)
        self._should_close_session = True

    async def close(self):
        if hasattr(self, '_request_model') and self._request_model:
            if hasattr(self._request_model, 'close'):
                await self._request_model.close()
            self._request_model = None

    def model_provider(self) -> str:
        return "siliconflow"

    def _invoke(self, model_name: str, messages: List[Dict], tools: List[Dict] = None, temperature: float = 0.1,
                top_p: float = 0.1, **kwargs: Any) -> AIMessage:
        return self._request_model._invoke(
            model_name=model_name, messages=messages, tools=tools,
            temperature=temperature, top_p=top_p, **kwargs)

    async def _ainvoke(self, model_name: str, messages: List[Dict], tools: List[Dict] = None, temperature: float = 0.1,
                       top_p: float = 0.1, **kwargs: Any) -> AIMessage:
        return await self._request_model._ainvoke(
            model_name=model_name, messages=messages, tools=tools,
            temperature=temperature, top_p=top_p, **kwargs)

    def _stream(self, model_name: str, messages: List[Dict], tools: List[Dict] = None, temperature: float = 0.1,
                top_p: float = 0.1, **kwargs: Any) -> Iterator[AIMessageChunk]:
        return self._request_model._stream(
            model_name=model_name, messages=messages, tools=tools,
            temperature=temperature, top_p=top_p, **kwargs)

    async def _astream(self, model_name:str, messages: List[Dict], tools: List[Dict] = None, temperature:float = 0.1,
               top_p:float = 0.1, **kwargs: Any) -> AsyncIterator[
        AIMessageChunk]:
        async for chunk in self._request_model._astream(
            model_name=model_name, messages=messages, tools=tools,
            temperature=temperature, top_p=top_p, **kwargs):
            yield chunk
```

其中RequestChatModel是内部实现的HTTP请求的模型实现类。其_invoke的实现遵循以下步骤：处理模型请求参数、发送模型请求、处理模型返回的结果数据，用户可以参考此流程自行实现。示例代码如下：

```python
from typing import List, Dict, Any
from openjiuwen.core.utils.llm.messages import AIMessage, UsageMetadata

# 处理模型请求参数
def _request_params(self, model_name: str, temperature: float, top_p: float, messages: List[Dict],
                    tools: List[Dict] = None, **kwargs: Any) -> Dict:
    params = {
        "model": model_name,
        "messages": messages,
        "temperature": temperature,
        "top_p": top_p,
        **kwargs
    }

    if tools:
        params["tools"] = tools

    return params


# 处理模型返回的结果数据
def _parse_response(self, response_data: Dict) -> AIMessage:
    """parse response"""
    choice = response_data.get("choices", [{}])[0]
    message = choice.get("message", {})

    return AIMessage(
        content=message.get("content", ""),
        tool_calls=message.get("tool_calls", []),
        usage_metadata=UsageMetadata(
            model_name=self.model_name,
            finish_reason=choice.get("finish_reason", ""),
            total_latency=response_data.get('usage', {}).get('total_tokens', 0)
        )
    )


def _invoke(self, model_name: str, messages: List[Dict], tools: List[Dict] = None, temperature: float = 0.1,
            top_p: float = 0.1, **kwargs: Any) -> AIMessage:
    messages = self.sanitize_tool_calls(messages)
    params = self._request_params(model_name=model_name, temperature=temperature, top_p=top_p,
                                  messages=messages, tools=tools, **kwargs)
    # 发送模型请求
    response = self.sync_client.post(
        verify=True,  # SSL校验开关默认打开，只在本地测试场景可以关闭
        url=self.api_base,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        },
        json=params,
        allow_redirects=False,
        timeout=self.timeout
    )
    response.raise_for_status()
    self.close_session()
    return self._parse_response(model_name, response.json())
```

## 使用自定义模型类

使用自定义模型类参见[接入大模型-ModelFactory方式使用模型](../基础功能/接入大模型.md#调用模型)章节，`model_provider`为自定义实现类的类名，在本示例中为siliconflow。
