不同模型在推理能力、对话流畅性、多轮交互等方面各有特色，用户可根据具体的应用场景灵活选择最适合的模型，例如在需要复杂逻辑推理的场景下选择推理能力更强的模型，在追求自然对话体验的场景下选择交互更自然的模型。
openJiuwen 提供了以下三种模型接入方式：

- 厂商系列模型接入：全面支持硅基流动厂商系列模型、遵循OpenAI接口系列模型的接入，用户可根据不同模型的特色（如推理能力、对话流畅性等）灵活选择适合业务场景的模型。通过`ModelFactory`提供了对大模型的统一调用接口。
- OpenAI 格式模型接入：提供`OpenAIChatModel`的封装接口，用户可直接按照`OpenAI`接口的使用方式进行初始化，模型服务需按照标准的`OpenAI`格式进行封装实现。
- 自定义模型接入：通过实现`ModelFactory`提供的扩展接口，用户可以将自有模型或第三方模型无缝集成进来，从而提升系统的灵活性和适应性。自定义模型接入将在高阶用法中介绍。

# 使用ModelFactory方式调用模型

## 初始化模型

使用 ModelFactory().get_model() 方法可获取模型实例。用户只需在参数配置中指定以下必填参数：

- 模型提供商标识符`model_provider`（目前支持 siliconflow、OpenAI 两种厂商）；
- 模型请求 URL 地址`api_base`；
- 模型鉴权 Token`api_key`。

此外，还可选填调用失败时的最大重试次数`max_retries`和单次调用大模型的超时时间`timeout`。

以siliconflow为例获取模型实例示例如下：

```python
from openjiuwen.core.utils.llm.model_utils.model_factory import ModelFactory

model = ModelFactory().get_model(
    model_provider="siliconflow",
    api_base="your path to model service",
    api_key="sk-****************************",
    max_retries=3,
    timeout=60
)
```

> **说明**
> 用户需要自行前往硅基流动或OpenAI的官网注册账号，以便获取模型广场中可用模型的`api_key`和模型调用的URL请求地址`api_base`。

## 准备模型输入

大模型支持以下三个主要输入参数，仅配置 `messages` 时，模型将基于上下文直接生成回复；同时配置 `messages` 和 `tools` 时，模型将结合上下文信息与工具定义，判断是否调用工具以完成用户请求。

- ​**model_name**​：标识当前请求的模型名称，用于指定调用的具体模型，为**必选**参数。
- ​**messages**​：按对话顺序排列的消息列表，每个消息对象包含发送者的角色（`role`）和消息内容（`content`），为**必选**参数。入参支持两种常见写法（List[BaseMessage]、List[Dict]）：

1. 当消息类型为List[BaseMessage]时，示例代码如下：

```python
from openjiuwen.core.utils.llm.messages import SystemMessage, HumanMessage

messages = [
    SystemMessage(content='你是一个AI助手'),
    HumanMessage(content='你好')
]
```

2. 当消息类型为List[Dict]时，示例代码如下：

```python
messages = [
    {"role": "system", "content": "你是一个AI助手"},
    {"role": "user", "content": "你好"}
]
```

- ​**tools**​：大模型可使用的工具列表，每个工具使用 JSON Schema 格式定义，详细说明所需参数、参数类型及是否必填等，为**可选**参数，仅在需要执行工具调用时使用。`tools`示例代码如下：

```python
# 工具定义
tools = [{
     "type": "function",
     "function": {
         "name": "get_weather",
         "description": "Get current weather for a location",
         "parameters": {
             "type": "object",
             "properties": {
                 "location": {
                     "type": "string",
                     "description": "City and country e.g. Paris, France"
                 },
                 "units": {
                     "type": "string",
                     "enum": ["metric", "imperial"],
                     "description": "Temperature unit"
                 }
             },
             "required": ["location"]
         }
     }
 }]
```

messages是面向大模型API构建的​多轮对话上下文数组，按固定的`system → user → assistant → tool`角色顺序，承载了系统指令、用户输入、模型回复与工具结果，为每一次推理提供完整、结构化的语义背景：

- `system`：代表系统或开发者的指令，用于​设定大模型的行为、背景知识或全局规则。对应的BaseMessage是`SystemMessage`。
- `user`：代表​终端用户的输入​，即用户提出的问题、指令或对话内容。对应的BaseMessage是`HumanMessage`。
- `assistant`：代表LLM的响应​，即系统根据用户输入和上下文生成的回复。对应的BaseMessage是`AIMessage`。
- `tool`：代表​外部工具或函数的调用结果，通常用于函数调用（Function Calling）场景。对应的BaseMessage是`ToolMessage`。

此外，还可选填以下参数：

- temperature：生成式模型的超参数，可用于控制生成语言模型中生成文本的随机性和创造性。它用于调整模型的softmax输出层中预测词的概率。当取值较低时输出更确定，取值较高时随机性和创造性增强。
- top_p：生成式模型在解码阶段使用的一个采样策略参数，用来动态截断概率分布，从而控制输出内容的随机性与多样性。当top_p值较小时结果更集中，top_p值增大时随机性和多样性提升。

## 调用模型

大模型的调用支持同步非流式`invoke`、异步非流式`ainvoke`、同步流式`stream`和异步流式`astream`四种类型：

- 同步非流式`invoke`：适用于对延迟不敏感、需要一次性拿到完整结果、且调用链本身已是同步阻塞的场景。
- 异步非流式`ainvoke`：适用于需要一次性拿全结果再处理、对实时性要求不高的场景。
- 同步流式`stream`：适用于需要实时、持续地单向推送数据，并且整个处理链路必须在同一线程内按顺序、低延迟完成的场景。
- 异步流式`astream`：适用于需要高并发实时推送、异步非阻塞输出，或边生成边展示的场景。

以同步非流式invoke和异步流式astream为例，介绍调用模型方式及运行结果，其他方式请参见API接口文档`openjiuwen.core.utils.llm`。

### 同步非流式invoke

通过`model.invoke`一次性获取模型响应的批输出结果。定义system提示词和用户输入，咨询与天气相关问题，预期调用工具，示例代码如下：

```python
import os
from openjiuwen.core.utils.llm.model_utils.model_factory import ModelFactory
from openjiuwen.core.utils.llm.messages import SystemMessage, HumanMessage

os.environ["LLM_SSL_VERIFY"] = "false"

def invoke():
    # 获取ModelFactory实例
    factory = ModelFactory()

    # 获取模型
    model = factory.get_model(
        model_provider="siliconflow",
        api_base="your path to model service",
        api_key="sk-****************************"
    )

    # 准备模型输入数据，定义system提示词和用户输入，咨询与天气相关问题，预期调用工具
    messages = [
        SystemMessage(content="你是一个AI助手").model_dump(exclude_none=True),
        HumanMessage(content="杭州天气").model_dump(exclude_none=True)
    ]
    # 天气工具schema定义
    tools = [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City and country e.g. Paris, France"
                    },
                    "units": {
                        "type": "string",
                        "enum": ["metric", "imperial"],
                        "description": "Temperature unit"
                    }
                },
                "required": ["location"]
            }
        }
    }]

    # 调用模型
    response = model.invoke(model_name="your_model", messages=messages, tools=tools, temperature=0.7, top_p=0.95)
    print(response)


if __name__ == "__main__":
    invoke()
```

输出结果如下：

```python
role='assistant' content='' name=None tool_calls=[ToolCall(id='019afe192ceee268d980f8acd98ccbc1', type='function', name='get_weather', arguments='{"location": "杭州, 中国"}')] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='Qwen/Qwen3-32B', finish_reason='tool_calls', total_latency=224.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=None
```

### 异步流式`astream`

通过`model.astream`实现对模型响应的流式输出，并通过异步执行，不阻塞当前线程。示例代码如下：

```python
import os
import asyncio
from openjiuwen.core.utils.llm.model_utils.model_factory import ModelFactory
from openjiuwen.core.utils.llm.messages import SystemMessage, HumanMessage

os.environ["LLM_SSL_VERIFY"] = "false"

async def astream():
    try:
        # 获取ModelFactory实例
        factory = ModelFactory()

        # 获取模型
        model = factory.get_model(
            model_provider="siliconflow",
            api_base="your path to model service",
            api_key="sk-****************************"
        )

        # 准备模型输入数据，定义system提示词和用户输入，咨询与天气无关问题，预期不调用工具，直接回答用户问题
        messages = [
            SystemMessage(content="你是一个AI助手").model_dump(exclude_none=True),
            HumanMessage(content="你好").model_dump(exclude_none=True)
        ]

        # 天气工具schema定义
        tools = [{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "City and country e.g. Paris, France"
                        },
                        "units": {
                            "type": "string",
                            "enum": ["metric", "imperial"],
                            "description": "Temperature unit"
                        }
                    },
                    "required": ["location"]
                }
            }
        }]

        # 使用async for遍历异步迭代器
        async for chunk in model.astream(model_name="your_model", messages=messages, tools=tools, temperature=0.7, top_p=0.95):
            print(chunk)

    except Exception as e:
        print(f"Error in async test: {str(e)}")
        raise
    finally:
        if model:
            await model.close()


# 定义一个异步函数main
async def main():
    # 调用异步函数test_async_astream()，并使用await等待其执行完成
    await astream()


if __name__ == "__main__":
    asyncio.run(main())
```

输出结果如下：

```python
role='assistant' content='你好' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='！' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='有什么' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='我可以' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='帮' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='你的' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='吗' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='？' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
role='assistant' content='' name=None tool_calls=[] usage_metadata=UsageMetadata(code=0, errmsg='', prompt='', task_id='', model_name='', finish_reason='stop', total_latency=0.0, model_stats={}, first_token_time='', request_start_time='') raw_content=None reason_content=''
```

# 使用OpenAIChatModel接入模型

## 初始化模型

使用OpenAIChatModel接入模型，在初始化模型时，需要配置如下参数：  
- 必选参数：用户需要配置以下两个必选参数来完成模型的初始化，成功创建一个模型实例。
  - **`api_key`**：模型鉴权 token，用于身份验证。  
  - **`api_base`**：模型请求的 URL 地址。  

- 可选参数：在初始化时，还可以根据需求配置以下可选参数：
  - **`max_retries`**：调用接口失败时的最大重试次数，用于提升调用的稳定性。  
  - **`timeout`**：单次调用大模型请求的超时时间，用于避免长时间等待。  

```python
from openjiuwen.core.utils.llm.model_utils.default_model import OpenAIChatModel

chat_model = OpenAIChatModel(
    api_key="your_api_key_here",  # 替换为实际的API密钥
    api_base="**********",  # API地址
    max_retries=3,
    timeout=60
)
```

> **说明**
> 用户需要自行前往兼容OpenAI格式的模型服务厂商进行注册账号，以便获取api_key和模型调用的URL请求地址api_base。

## 准备模型输入

准备模型输入参见本章节[使用ModelFactory方式使用模型-准备模型输入](#准备模型输入)模块内容。

## 调用模型

调用模型参见本章节[使用ModelFactory方式使用模型-调用模型](#调用模型)模块内容，配置的`model_name`为`api_base`服务地址支持的模型名称。
